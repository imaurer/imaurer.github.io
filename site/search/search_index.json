{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ian Maurer's Notes","text":"<p>Personal website for Ian Maurer, CTO of GenomOncology.</p>"},{"location":"#articles","title":"Articles","text":"<ul> <li>What is a Custom GPT?</li> <li>Using grammars to constrain llama.cpp output</li> <li>LLMs just don't understand</li> <li>The Answer is the Easy Part</li> </ul>"},{"location":"#videos","title":"Videos","text":"<ul> <li>Focus on Talent Podcast - March 2024</li> <li>Ci4CC: Demystifying LLM Jargon - February 2024</li> <li>Podcast: Talk Python to Me #456 - January 2024</li> <li>Ci4CC: Generative AI Updates - November 2023</li> <li>Grounding Medical Q&amp;A using ChatGPT plugins and Knowledge Graphs - June 2023</li> <li>Clinco-Genomic Data Analysis using Metabase and GenomOncology - March 2023</li> <li>What is a VCF File - January 2023</li> <li>Talk Python to Me #154 - February 2018</li> </ul>"},{"location":"llama-cpp-grammars/","title":"Using grammars to constrain llama.cpp output","text":"<p>September 6, 2023</p>"},{"location":"llama-cpp-grammars/#using-grammars-to-constrain-llamacpp-output","title":"Using grammars to constrain llama.cpp output","text":"<p>Context-free grammars have increased the accuracy of my large language model-based biomedical data extraction pipeline.</p> <p>The llama.cpp project, which is a high-performance library for running LLMs locally on CPUs, GPUs, and Apple\\u2019s Metal graphics platform (e.g M1, M2), has recent added the support of grammars to guide and constrain the output of the LLM.</p> <p>A grammar is a notation that describes the valid syntax of text.</p> <p>The GGML grammar notation (GBNF) is documented here and there are example grammars for generic JSON, C programming language, and chess moves.</p> <p>I have gotten pretty good at crafting my grammars by hand, but these tools are helpful for getting started:</p> <ul> <li>Web UI: Grammar Builder (input: TS types)</li> <li>llama.cpp script: json-schema-to-grammar.py (input: jsonschema file)</li> </ul> <p>For Python usage, this capability was exposed in the llama-cpp-python project starting in version 0.1.78.</p> <p>To use it, there is a class called LlamaGrammar that is passed into your LLM instance on inference:</p> <pre><code>from llama_cpp.llama import Llama, LlamaGrammar\ngrammar = LlamaGrammar.from_string(grammar_text)\nllm = Llama(model_path)\nresponse = llm(prompt, grammar=grammar)\n</code></pre> <p>LlamaGrammar also has a <code>from_file</code> helper function.</p> <p>Grammars work by guiding and constraining the LLM as it is predicting the next token.</p> <p>This feature eliminates the challenges with trying to force the model to generate well-formed JSON via prompt engineering or via post-processing on the response text.</p> <p>In addition to guaranteeing the output, the overall quality and accuracy of the underlying response \"logic\" improves as well. The grammar acts like guardrails in bowling which not only prevents gutter balls (i.e. not well-formed JSON) but also increases the likelihood of a strike (i.e. the correct answer).</p>"},{"location":"llms-just-dont-understand-2/","title":"LLMs Just Don't Understand","text":"<p>July 14, 2023</p>"},{"location":"llms-just-dont-understand-2/#llms-just-dont-understand","title":"LLMs Just Don't Understand","text":"<p>I agree with Grady's tweet:</p> <p></p> <p>But I also understand (at least I think, I think I do), that it's difficult to describe the subtleties of LLMs and their capabilities at language manipulation and automation without resorting to anthropomorphism at times.</p> <p>So, I decided to manipulate some tokens using my word calculator to come up with some alternative terms:</p> <p>Here is the information organized into a Markdown table:</p> Concept LLM Capability Alternate Terminology Hallucinations Produces outputs that don't align with established reality, creating novel or unusual responses. Novelty Creation Reasoning Provides outputs that follow a logical structure based on the context of the input. Logic Simulation Understanding Generates contextually relevant outputs by processing the semantic structure of the input. Contextual Response Learning Produces new responses by applying patterns acquired during training. Pattern Use Memory Uses information embedded in training data to generate responses that seem contextually appropriate. Data Recall Judgement Produces outputs that appear to evaluate or weigh different factors or options. Evaluation Simulation Decision-making Selects the most fitting output from a range of possibilities based on the input. Optimal Selection Reading Processes and understands the structure and semantics of written language. Text Interpretation Writing Produces written content that follows grammatical rules and matches the context of the input. Text Production Imagination Generates narratives or scenarios that extend beyond the given input. Scenario Creation Thinking Generates outputs that seem to display a thought process, achieved by pattern matching based on training data. Simulated Thought Process Translation Converts a piece of text from one language to another, maintaining the meaning and context. Language Conversion Listening In the context of voice recognition software, transcribes spoken language into written text. Audio Transcription Speaking In the context of text-to-speech software, converts written text into spoken language. Speech Synthesis <p>Would you like to add or modify any entries in this table?</p> <p>I honestly don't like them because they are clunky and hard to remember.</p> <p>But I now have a blog article to point to when I see smart people arguing over definitions.</p>"},{"location":"the-answer-is-the-easy-part/","title":"The Answer is the Easy Part","text":"<p>May 18, 2023</p>"},{"location":"the-answer-is-the-easy-part/#the-answer-is-the-easy-part","title":"The answer is the easy part","text":"<p>Med-PaLM 2 made news by scoring 86.5% on the MedQA dataset setting a new state-of-the-art. A great new tool for practitioners, but the truly difficult parts remain.</p> <p>Not only did Med-PaLM2 become the leader on the MedQA-USMLE benchmark, it was also reported that \"physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility.\"</p> <p>Even in the middle of a typhoon of very impressive results, this appears to be an AlexNet moment type jump for this benchmark. (Side note: exponential curves make for humorous time-based x-axes, note the gaps in months are 9, 6, 7, 2, 0, 0, 3_)_</p> <p></p> <p>However, the answer is the easy part. It's just a prediction made by a machine based on what it read on the internet.</p> <p>What remains?</p> <ol> <li>Identifying what is missing or unknown.</li> <li>Formulating a clear, correct and complete question.</li> <li>Some typing, speaking or eventually neural-linking.</li> <li>Reading and understanding the meaning of an answer.</li> <li>Validating the correctness of an answer.</li> <li>Determining all possible options based on an answer.</li> <li>Evaluating all these options considering the full spectrum of human values.</li> <li>Communicating these options to other humans in a clear, emotionally appropriate manner.</li> <li>Collaborating with others to decide the best course of action.</li> <li>Some more typing, speaking or eventually neural-linking.</li> </ol> <p>See, lots of good stuff for humans to do with their shiny new tools!</p> <p>Asking good questions will continue to differentiate experts from non-experts and humans from AI. Because, to ask really good questions you need to know what is missing and you need to care that it is.</p>"},{"location":"what-is-a-custom-gpt/","title":"What is a Custom GPT?","text":""},{"location":"what-is-a-custom-gpt/#jan-8-2024","title":"Jan 8, 2024","text":"<p>Custom GPTS are a configurable, shareable chat experience available to ChatGPT plus subscribers. Custom GPTs were announced on November 6<sup>th</sup>, 2023 at their inaugural Dev Day event and the GPT Store was announced on January 10<sup>th</sup>, 2024.</p> <p></p> <p>In this article:</p> <ul> <li>What makes up a Custom GPT?</li> <li>Product Announcements Leading up to GPTs</li> <li>Key Benefits of GPTS</li> <li>Key Risks of GPTS</li> <li>Why create a GPT?</li> <li>Further Reading</li> </ul>"},{"location":"what-is-a-custom-gpt/#what-makes-up-a-custom-gpt","title":"What makes up a Custom GPT?","text":"<p>Custom GPTs have the following properties configurable by its creator directly in the GPT Editor:</p> <ul> <li>Name</li> <li>Logo</li> <li>Description</li> <li>Custom Instructions</li> <li>Conversation Starters (max. 4)</li> <li>Knowledge Retrieval<ul> <li>Maximum 10 files</li> <li>Maximum 512MB per file</li> </ul> </li> <li>Optional Access to OpenAI \"Capabilities\":<ul> <li>Web Browsing</li> <li>DALL-E Image Generation</li> <li>Code Interpreter</li> </ul> </li> <li>Actions<ul> <li>Schema (via OpenAPI specification)</li> <li>Authentication Setup (OAuth or Token-based)</li> <li>Privacy Policy URL (for shared GPTs with Actions)</li> </ul> </li> <li>Sharing<ul> <li>Publish: Only you, Anyone with Link, Everyone</li> <li>Public, Shareable URL for non-private GPTs</li> </ul> </li> </ul>"},{"location":"what-is-a-custom-gpt/#product-announcements-leading-up-to-custom-gpts","title":"Product Announcements Leading up to Custom GPTS","text":"<p>ChatGPT Plugins (March 23, 2023): Plugins allowed ChatGPT to call REST API endpoints which demonstrated a very powerful paradigm of language model \"tool usage\". Unfortunately, the discoverability and usability of plugins for both developers and users of plugins was subpar, leading to a lack of product-market fit. Plugins have been refashioned as Actions in GPTs.</p> <p>Code Interpreter (March 23, 2023): Originally released as a plugin, the code interpreter is a sandboxed version of the Python interpreter that enables the creation and running of scripts. Ideally suited as a low-code data analysis tool, it is still available as a GPT capability and as a standalone GPT called Data Analyst.</p> <p>Web Browser (March 23, 2023): Originally released as a plugin, this enabled ChatGPT to search the open web and bring back content for analysis. This capability remains in all standard chats, as a GPT capability and as a standalone GPT called Web Browser.</p> <p>ChatGPT app for iOS (May 18, 2023): The native ChatGPT app was released to iOS in May and rolling out to Android customers starting in July.</p> <p>Custom Instructions (July 20, 2023): Custom instructions allowed a user to provide information about themselves and direction on how they want the chatbot to respond. Originally released as a beta feature, they are now a core setting. In the announcement, it was casually mentioned that custom instructions could improve the experience working with plugins which appears to be a critical insight into the development of GPTS.</p> <p>See, Hear and Speak (September 25, 2023): Powered by GPT-4V(ision) for seeing, Whisper for hearing and Text-to-Speech (TTS) for speaking, these multi-modality capabilities make the mobile experience especially feel much more powerful. Having a conversation your phone about pictures you take feels straight out of the movie Her.</p> <p>DALL-E 3 (October 19, 2023): OpenAI's image generation model was embedded directly into ChatGPT. The model has been trained to not create images in the style of living artists and to not violate copyright. However, users quickly identified hacks to get around these safeguards.</p> <p>Assistants API (November 6, 2023): Launched on Dev Day with GPTS, the Assistants API allows developers to create assistants within their own application. The API include threads for handling the context of a long conversation along with hosted knowledge for retrieval augmented generation (RAG). Assistants can also access the code interpreter and return function calling JSON responses.</p> <p>Actions (November 6, 2023): OpenAI simplified and renamed Plugins as Actions and made them available within GPTs. Like Plugins, Actions are defined by an OpenAPI specification and support multiple authentication approaches. Actions can be declared to be \"consequential\" or not by the developer and this flag is used to inform the user before any consequential actions are taken.</p> <p>Custom GPTS (November 6, 2023): Actions were then bundled into a shareable, re-usable \"chat template\" configured by Custom Instructions and augmented with the OpenAI tool suite of browsing, image generation, code interpreter, vision, speech recognition, and text-to-speech.</p>"},{"location":"what-is-a-custom-gpt/#key-benefits-of-custom-gpts","title":"Key Benefits of Custom GPTS","text":"<p>Some of the key benefits of the Custom GPTS over standard GPTs and Plugins.</p> <ul> <li>Custom Instructions<ul> <li>Available at the \"chat template\" level rather than global, account-level.</li> <li>Empowers user to switch model context without managing a system prompt library.</li> <li>Enables developer to direct model on how to best call a mix of custom Actions.</li> </ul> </li> <li>Sharable<ul> <li>Custom GPTs can be private but also shared with via URL.</li> <li>OpenAI GPT store should drive discoverability and thus increase both demand and creation of higher quality GPTs.</li> </ul> </li> <li>Easy to Start<ul> <li>No coding experience required to build a basic GPT without actions.</li> <li>Editor includes a chatbot where you can build the Custom GPT through conversation.</li> <li>DALL-E can generate a custom icon for your Custom GPT based on your name and description.</li> </ul> </li> <li>Cost Model<ul> <li>No additional cost to developing and using Custom GPTs for ChatGPT plus subscribers.</li> <li>Unlike building a typical \"AI application\", there is no escalating usage-based fees.</li> </ul> </li> </ul>"},{"location":"what-is-a-custom-gpt/#key-risks-of-custom-gpts","title":"Key Risks of Custom GPTs","text":"<p>Here are some of the key risks I am considering with regards to building a Custom GPT:</p> <ul> <li>Platform Risk<ul> <li>Sherlocking: OpenAI could adopt your features into the platform.</li> <li>Exclusion: OpenAI could ban your GPT for whatever reason.</li> <li>Abandonment: OpenAI could abandon this initiative if it doesn't achieve product-market fit.</li> </ul> </li> <li>Adversarial Prompting<ul> <li>Prompt Injection is still an unsolved problem.<ul> <li>Don't build anything that takes non-revokable and consequential actions.</li> <li>Assume if your Action's API has valuable data and a security hole, someone will be able to convince the chatbot to exploit that hole.</li> </ul> </li> <li>Anything uploaded to OpenAI currently can be downloaded by other users.<ul> <li>Custom Instructions are easily extracted, there are multiple GitHub repositories sharing the techniques and the actual prompts used by GPTs</li> <li>Knowledge files uploaded to Custom GPTs have reportable been directly downloaded and directly quoted directly in chat.</li> </ul> </li> </ul> </li> <li>No Moat<ul> <li>OpenAI, like the other large, centralized, frontier model providers may have no moat.</li> <li>Algorithmic and data quality improvements discovered by open-source die hards will continue to chip away at lead that the frontier models started with in 2023.</li> <li>Small, decentralized, fine-tuned, open-source models that can run locally or even on-device may win due to their task-specific accuracy, cost/compute profiles and data sovereignty benefits.</li> </ul> </li> <li>LLM Reliability<ul> <li>Building a product inside of an autoregressive chatbot is a fundamentally dubious proposition at this point given their current levels of reliability.</li> <li>GPT-4, the most capable model, hallucinates and is susceptible to prompt injection.</li> <li>Improved models that hallucinate less may simply emit harder-to-detect errors that only benefit those with domain expertise in the first place.</li> <li>Frontier models may have plateaued in their capabilities due to scaling limits and/or costs of the current architecture.</li> </ul> </li> </ul>"},{"location":"what-is-a-custom-gpt/#why-create-a-custom-gpt","title":"Why create a Custom GPT?","text":"<p>Given the above benefits and risks, there are still some risk-less opportunities to consider when trying to decide whether to create a GPT. To name a few:</p> <p>Task-specific, No-Code Custom GPT for Personal Use: It's very easy to create a custom GPT tailored for a very specific problem. If you use the GPT a few times to solve a specific problem more efficiently, then there is a chance you will have broken even with your time investment.</p> <p>Retrieval Augmented Generation (RAG) Prototyping: Developing a RAG-based application seems easy until you actually try it. If all your application does is split up documents into paragraphs, embeds them into a vector and uses cosine similarity to retrieve documents based on a question, then either it's not going to work or it wasn't a very difficult problem to begin with. Using GPTS to do a quick prototype might save you some time and it's a cheaper approach than having a developer build something with LangChain or LlamaIndex. It also doesn't cost anything to host the knowledge files unlike the Assistants API which costs 20 cents per GB per day. (Note: OpenAI has provided very little detail thus far about how knowledge chunking, indexing and retrieval actually works and if you have a more advanced use case, their current default approach probably won't help you much)</p> <p>Focus Effort on Actions: Recognize that all of the information uploaded to OpenAI can be exfiltrated via prompt hacking. If you concentrate on creating value behind your API, then that value will be both controllable and transferrable to other products or projects. In the future, something like what people are calling \"agents\" will be a thing and those things will like to call APIs like yours to solve problems. Use GPTS to figure out how this might work for you in the future.</p> <p>Collect Users via Authentication: Custom GPTs support authentication, which allows you to collect user information such as an email address. Gathering contact information of people interested in your (potential or actual) product and/or service is a no brainer.</p> <p>Plan an Open Source Migration: There is Dify.AI, which has an Apache 2.0 licensed code base on GitHub, and there will be others. So, if you build something cool and you want it to run on your infrastructure, using the models you control, then there will likely be a reasonable escape hatch if your time with OpenAI ends badly.</p> <p>Have Fun Learning Something New: When nothing is left, chalk it up to learning something new and move on.</p>"},{"location":"what-is-a-custom-gpt/#further-reading","title":"Further Reading","text":"<p>Prior to the GPT Store, there were so many Custom GPT directories that someone has made a list of Custom GPT lists.</p> <p>Simon Willison's posted about Custom GPTs the week after they were announced and per usual his initial notes and perspective hold up extremely well. I want to disagree with his initial take which is that GPTs are \"not much more than ChatGPT in a trench coat fancy wrapper for standard GPT-4 with some pre-baked prompts.\" Unfortunately, most of the GPTs I have played with fit this description exactly.</p> <p>Just this weekend, Bram Adams asked \"Are GPTs are websites?\" in his newsletter. His ideas around \"Smart Forms\", \"Local Information Load Balancer\" and GPTs being a \"subject matter expert layer\" are all interesting to me.</p> <p>Nick Dobos of Mind Goblin Studios has created the best developer GPT I have tried called Grimoire definitely worth playing with even if you are not a developer. Nick has shares some insightful tweets about his work.</p>"}]}
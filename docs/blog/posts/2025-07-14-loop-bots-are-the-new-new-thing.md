---
date: 2025-07-14
categories: ['Technology']
tags: ['ai', 'llm', 'claude']
authors: ['imaurer']
---
# Loop Bots are the new, new thing

I've been writing a lot of software using [Claude Code](https://claude.ai/docs).

I'm also increasingly using Claude Code for non-coding tasks. In fact, I've rearranged my Obsidian workflow so Claude and I can easily collaborate on the same set of documents.

I think of this Obsidian vault that Claude Code and I share as it's "long-term memory". 

Claude can edit entire documents or specific sections marked by inline hashtags—an idea inspired by the [aider project](https://aider.chat/docs/usage/watch.html). This integration uses [Obsidian sync](https://obsidian.md/sync) and a monitoring script Claude helped me set up in Python, running on an old iMac with Ubuntu in my basement.

##### One-Shot: Print and Exit

Calling Claude Code and [Gemini CLI](https://deepmind.google/gemini-cli) in non-chat mode using the `-p` flag has largely replaced my previous approaches of either single-prompt calls using Simon Willison’s excellent [`llm`](https://simonwillison.net/llm/) tool or writing custom Python scripts with a variety of other Python libraries that I never got too attached too.

Since these CLIs are essentially free, I use them much more frequently and with much larger context. I use them for tasks like reviewing a full small/medium codebases. Something that is possible with Simon's `llm` but unfortunately Simon doesn't provide a Max plan or subsidize my token consumption habits.

*(Quick Tip: Ask for a [SWOT analysis](https://en.wikipedia.org/wiki/SWOT_analysis) when doing code reviews (or any review) to [reduce model sycophancy](https://openai.com/index/sycophancy-in-gpt-4o/).)*

#### Cost

I prefer Gemini for these one-shot, print and exit tasks, due to its long-context support (1M tokens), their [generous free plan](https://www.techtarget.com/searchsoftwarequality/news/366626444/Google-touts-free-tier-for-multimodal-AI-terminal), and to conserve my Claude tokens for long-running, multi-step dev tasks.

I've subscribed to the [Claude Max 20x](https://www.anthropic.com/max) plan, reducing the stress of token-based payment. 

However, even at 20x, I still regularly exhaust my Opus tokens and occasionally use up all my Sonnet tokens for a day as well.

#### My 4 Modes of Using LLMs

1. **o3 (Pro) in Chat UI:** Disposable brainstorming and search. I turn off memory. o3 is the best, raw problem solver. But I am using it less and less now that it is the least convenient way of getting my data into it's context.

2. **Multi-step Obsidian Doc Updates:** I create and edit documents on my phone, iPad, or desktop. I trigger Claude via file properties (YAML frontmatter) and inline hashtags. My monitor fires off Claude using the `-p` flag, Claude [slash commands](https://docs.anthropic.com/en/docs/claude-code/slash-commands) and [`--max-turns` flag](https://docs.anthropic.com/en/docs/claude-code/cli-reference). *Note: If I wasn't planning something bigger/better, I would have invested time into learning [Claude hooks](https://docs.anthropic.com/en/docs/claude-code/hooks) which are relatively new.*
 
3. **Single-shot CLI Calls:** Using the `-p` flag with Gemini CLI, I can pipe in context and run a prompt, useful for generating plans, reviewing code and other tasks where have 500-900K tokens in memory is useful. It makes a single document that I send directly to disk. Not sure if Gemini CLI supports multi-turn prompts since I don't use it like that right now.

4. **Claude Code:** Given a detailed plan (often generated by me & Gemini), "classic, chat-style" Claude Code executes took calls until completing my tasks. Sometimes Claude will run uninterrupted for upwards of an hour for me completing the task successfully. I always use Markdown documents to augment Claude Code's todo lists. If I don't, Claude will often lose the thread when it runs out of context and it does "compact".

##### What to Call This New, New Thing?

It's clear to me that these tools represent something new. Just like how ChatGPT felt like a brand new thing in November 2022.

Technically, they follow the pattern described in the [ReAct paper from 2022](https://arxiv.org/abs/2210.03629):

- Reason
- Act (Call a Tool)
- Observe
- Repeat
    
Yet, they feel new because they finally work exceptionally well, much like how models like o1/o3/R1 demonstrated how effective [Chain of Thought](https://arxiv.org/abs/2201.11903) is when the models does it by default without aggressive prompt engineering.

But what should we call these new systems that perform multi-step processes, either in chat or using the `-p` flag?

But what differentiates these things?

##### Not Reasoning and Tool Calling

Reasoning and tool-calling alone don't differentiate these tools—ordinary chatbots already reason, browse the web, read PDFs, and create images reasonably well.

##### Not CLI or Terminal

Terms like "CLI" or "terminal" won't remain meaningful for long.

The terminal interface is just a temporary convenience, enabling easy access to Unix-like tools for software development.

Eventually, these tools will integrate seamlessly into larger assistants or agents, leaving the CLI primarily for specialized use cases.

##### Not Agents or Assistants

I've seen terms like "coding agents" or "coding assistants," but I think these things currently delivered as command-line executables are more fundamental.

I don't anticipate people directly conversing with them for long. (I also think IDEs are on the way out, but that's a topic for another day.)

##### The Loop Is the Thing

The unique differentiator is the loop.

They cycle through reasoning, tool calling, and observing outcomes until their task is complete.

So I propose we call them **Loop Bots**. 

The next evolution of Chat Bots. Short, sweet, easy to remember.

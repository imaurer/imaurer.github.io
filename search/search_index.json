{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ian Maurer's Blog","text":""},{"location":"#2025","title":"2025","text":""},{"location":"#2024","title":"2024","text":""},{"location":"#what-is-a-custom-gpt","title":"What is a Custom GPT?","text":"<p>January 8, 2024</p> <p>Custom GPTS are a configurable, shareable chat experience available to ChatGPT plus subscribers. Custom GPTs were announced on November 6<sup>th</sup>, 2023 at their inaugural Dev Day event and the GPT Store was announced on January 10<sup>th</sup>, 2024.</p>"},{"location":"#2023","title":"2023","text":""},{"location":"#using-grammars-to-constrain-llamacpp-output","title":"Using grammars to constrain llama.cpp output","text":"<p>September 6, 2023</p> <p>Context-free grammars have increased the accuracy of my large language model-based biomedical data extraction pipeline. The llama.cpp project has recently added support for grammars to guide and constrain LLM output.</p>"},{"location":"#llms-just-dont-understand","title":"LLMs Just Don't Understand","text":"<p>July 14, 2023</p> <p>It's difficult to describe the subtleties of LLMs and their capabilities at language manipulation and automation without resorting to anthropomorphism at times. So, I decided to manipulate some tokens using my word calculator to come up with some alternative terms.</p>"},{"location":"#the-answer-is-the-easy-part","title":"The answer is the easy part","text":"<p>May 18, 2023</p> <p>Med-PaLM 2 made news by scoring 86.5% on the MedQA dataset setting a new state-of-the-art. A great new tool for practitioners, but the truly difficult parts remain.</p>"},{"location":"about/","title":"About Ian Maurer","text":"<p>I'm the Chief Technology Officer at GenomOncology, a precision oncology startup based in Cleveland, Ohio. I focus on building AI-powered systems that help oncologists make better treatment decisions using genomic data.</p>"},{"location":"about/#background","title":"Background","text":"<p>My career spans software engineering, data science, and biotechnology. I'm passionate about the intersection of artificial intelligence and genomics, particularly how AI can accelerate medical discoveries and improve patient outcomes.</p> <p>At GenomOncology, I lead the development of platforms that analyze genomic variants and connect them to therapeutic insights, helping oncologists understand complex molecular data and translate it into actionable treatment recommendations.</p>"},{"location":"about/#areas-of-expertise","title":"Areas of Expertise","text":"<p>AI &amp; Machine Learning - Large Language Models (LLMs) and their applications in healthcare - Biomedical data extraction and analysis - Constrained generation and grammar-based LLM guidance - AI tool integration and workflow automation</p> <p>Genomics &amp; Bioinformatics - HGVS variant notation and analysis - Precision oncology data platforms - Clinical genomics workflows - Biomedical knowledge graphs</p> <p>Software Architecture - High-performance systems design - Developer tooling and productivity - API design and integration - Modern web technologies</p>"},{"location":"about/#open-source-projects","title":"Open Source Projects","text":"<p>I maintain several open-source projects focused on developer productivity and bioinformatics:</p> <ul> <li>biomcp: Biomedical MCP toolkit for AI assistants</li> </ul>"},{"location":"about/#speaking-media","title":"Speaking &amp; Media","text":"<p>I regularly speak about AI, genomics, and healthcare technology:</p> <p>Recent Appearances: - Talk Python to Me #456 - Building GPT Actions with FastAPI (Jan 2024) - Focus on Talent Podcast - AI in Healthcare (Mar 2024) - Talk Python to Me #154 - Python in Biology and Genomics (Feb 2018)</p> <p>Technical Presentations: - Grounding Medical Q&amp;A using ChatGPT plugins and Knowledge Graphs (YouTube) - Clinco-Genomic Data Analysis using Metabase and GenomOncology (YouTube) - What is a VCF File (YouTube)</p>"},{"location":"about/#writing-philosophy","title":"Writing Philosophy","text":"<p>I write about the practical applications of emerging technologies, particularly AI and genomics. My posts tend to focus on:</p> <ul> <li>Real-world implementations rather than theoretical concepts</li> <li>The intersection of AI and domain-specific knowledge</li> <li>Developer tools and productivity enhancements</li> <li>The human elements that remain crucial as AI capabilities expand</li> </ul> <p>I believe that the most interesting technological advances happen at the intersection of disciplines - where AI meets biology, where developer tools meet domain expertise, and where theoretical capabilities meet practical constraints.</p>"},{"location":"about/#connect","title":"Connect","text":"<ul> <li>Twitter: @imaurer</li> <li>GitHub: imaurer</li> <li>LinkedIn: ianmaurer</li> </ul> <p>Feel free to reach out if you're interested in AI applications in healthcare, genomics, or developer productivity tools.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/","title":"The answer is the easy part","text":"<p>Med-PaLM 2 made news by scoring 86.5% on the MedQA dataset setting a new state-of-the-art. A great new tool for practitioners, but the truly difficult parts remain.</p> <p>Not only did Med-PaLM2 become the leader on the MedQA-USMLE benchmark, it was also reported that \"physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility.\"</p> <p>Even in the middle of a typhoon of very impressive results, this appears to be an AlexNet moment type jump for this benchmark. (Side note: exponential curves make for humorous time-based x-axes, note the gaps in months are 9, 6, 7, 2, 0, 0, 3_)_</p> <p></p> <p>However, the answer is the easy part. It's just a prediction made by a machine based on what it read on the internet.</p> <p>What remains?</p> <ol> <li>Identifying what is missing or unknown.</li> <li>Formulating a clear, correct and complete question.</li> <li>Some typing, speaking or eventually neural-linking.</li> <li>Reading and understanding the meaning of an answer.</li> <li>Validating the correctness of an answer.</li> <li>Determining all possible options based on an answer.</li> <li>Evaluating all these options considering the full spectrum of human values.</li> <li>Communicating these options to other humans in a clear, emotionally appropriate manner.</li> <li>Collaborating with others to decide the best course of action.</li> <li>Some more typing, speaking or eventually neural-linking.</li> </ol> <p>See, lots of good stuff for humans to do with their shiny new tools!</p> <p>Asking good questions will continue to differentiate experts from non-experts and humans from AI. Because, to ask really good questions you need to know what is missing and you need to care that it is.</p>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#the-human-elements-that-remain-critical","title":"The Human Elements That Remain Critical","text":"","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#clinical-intuition-and-experience","title":"Clinical Intuition and Experience","text":"<p>While Med-PaLM 2 can process vast amounts of medical literature and provide accurate answers to well-formed questions, it lacks the clinical intuition that comes from years of patient interaction. A seasoned physician can pick up on subtle cues\u2014a patient's demeanor, an unusual combination of symptoms, or an inconsistency in their story\u2014that might lead to asking entirely different questions.</p>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#understanding-context-and-stakes","title":"Understanding Context and Stakes","text":"<p>Medical decisions aren't made in a vacuum. They involve understanding:</p> <ul> <li>Patient preferences and values</li> <li>Quality of life considerations  </li> <li>Economic constraints</li> <li>Family dynamics</li> <li>Cultural factors</li> <li>Risk tolerance</li> </ul> <p>An AI can tell you the statistical best treatment option, but it can't weigh that against a patient's deeply held beliefs about end-of-life care or their financial situation.</p>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#the-art-of-medical-communication","title":"The Art of Medical Communication","text":"<p>Delivering medical information is as much art as science. The same prognosis might need to be communicated completely differently to:</p> <ul> <li>A 25-year-old software engineer who wants all the technical details</li> <li>An elderly patient who prefers simple, reassuring language</li> <li>A worried parent trying to understand their child's condition</li> <li>A family grappling with a terminal diagnosis</li> </ul>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#collaborative-decision-making","title":"Collaborative Decision Making","text":"<p>Modern medicine is increasingly collaborative, involving multiple specialists, patients, families, and care teams. Facilitating these conversations, managing conflicting opinions, and building consensus requires emotional intelligence and interpersonal skills that go far beyond pattern matching.</p>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#the-questions-behind-the-questions","title":"The Questions Behind the Questions","text":"<p>Perhaps most importantly, the best clinicians are excellent at identifying what questions need to be asked. They can:</p> <ul> <li>Recognize when standard protocols don't apply</li> <li>Identify gaps in available information</li> <li>Question assumptions and dig deeper</li> <li>Consider what might be missing from the current picture</li> </ul> <p>This meta-cognitive skill\u2014knowing what you don't know and caring enough to find out\u2014remains distinctly human.</p>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-05-18-the-answer-is-the-easy-part/#a-tool-not-a-replacement","title":"A Tool, Not a Replacement","text":"<p>Med-PaLM 2 and similar AI systems represent incredibly powerful tools that can augment human expertise. They can:</p> <ul> <li>Rapidly synthesize vast amounts of literature</li> <li>Provide comprehensive differential diagnoses</li> <li>Suggest evidence-based treatment options</li> <li>Flag potential drug interactions or contraindications</li> </ul> <p>But they work best when wielded by skilled practitioners who know how to ask the right questions, interpret the answers in context, and navigate the complex human elements of medical care.</p> <p>The future of medicine isn't AI replacing doctors\u2014it's AI amplifying the uniquely human aspects of healing while handling the information processing tasks that machines do better than humans.</p> <p>And that future looks pretty exciting to me.</p>","tags":["Med-PaLM","medical AI","USMLE","healthcare technology"]},{"location":"blog/posts/2023-07-14-llms-just-dont-understand/","title":"LLMs Just Don't Understand","text":"<p>I agree with Grady's tweet:</p> <p></p> <p>But I also understand (at least I think, I think I do), that it's difficult to describe the subtleties of LLMs and their capabilities at language manipulation and automation without resorting to anthropomorphism at times.</p> <p>So, I decided to manipulate some tokens using my word calculator to come up with some alternative terms:</p> <p>Here is the information organized into a Markdown table:</p> Concept LLM Capability Alternate Terminology Hallucinations Produces outputs that don't align with established reality, creating novel or unusual responses. Novelty Creation Reasoning Provides outputs that follow a logical structure based on the context of the input. Logic Simulation Understanding Generates contextually relevant outputs by processing the semantic structure of the input. Contextual Response Learning Produces new responses by applying patterns acquired during training. Pattern Use Memory Uses information embedded in training data to generate responses that seem contextually appropriate. Data Recall Judgement Produces outputs that appear to evaluate or weigh different factors or options. Evaluation Simulation Decision-making Selects the most fitting output from a range of possibilities based on the input. Optimal Selection Reading Processes and understands the structure and semantics of written language. Text Interpretation Writing Produces written content that follows grammatical rules and matches the context of the input. Text Production Imagination Generates narratives or scenarios that extend beyond the given input. Scenario Creation Thinking Generates outputs that seem to display a thought process, achieved by pattern matching based on training data. Simulated Thought Process Translation Converts a piece of text from one language to another, maintaining the meaning and context. Language Conversion Listening In the context of voice recognition software, transcribes spoken language into written text. Audio Transcription Speaking In the context of text-to-speech software, converts written text into spoken language. Speech Synthesis <p>Would you like to add or modify any entries in this table?</p> <p>I honestly don't like them because they are clunky and hard to remember.</p> <p>But I now have a blog article to point to when I see smart people arguing over definitions.</p>","tags":["LLM","understanding","anthropomorphism","terminology"]},{"location":"blog/posts/2023-07-14-llms-just-dont-understand/#the-challenge-of-describing-ai","title":"The Challenge of Describing AI","text":"<p>The fundamental challenge we face when discussing AI capabilities is that our language evolved to describe human cognition and behavior. When we apply these same terms to artificial systems, we inevitably import assumptions about consciousness, intentionality, and understanding that may not apply.</p> <p>Consider the word \"understanding.\" When we say a human understands something, we imply:</p> <ul> <li>Conscious awareness of the concept</li> <li>Ability to apply knowledge flexibly across contexts</li> <li>Emotional or experiential connection to the meaning</li> <li>Integration with a broader worldview and personal experience</li> </ul> <p>When an LLM produces contextually appropriate responses, it demonstrates something that looks like understanding from the outside, but the internal mechanisms are fundamentally different.</p>","tags":["LLM","understanding","anthropomorphism","terminology"]},{"location":"blog/posts/2023-07-14-llms-just-dont-understand/#why-this-matters","title":"Why This Matters","text":"<p>This isn't just semantic nitpicking. The language we use shapes how we think about AI capabilities and limitations. When we say an AI \"understands\" or \"thinks,\" we risk:</p> <ol> <li>Overestimating capabilities: Assuming the AI has human-like comprehension when it's actually pattern matching</li> <li>Underestimating limitations: Missing edge cases where the pattern matching fails</li> <li>Misaligned expectations: Building systems based on false assumptions about AI behavior</li> </ol>","tags":["LLM","understanding","anthropomorphism","terminology"]},{"location":"blog/posts/2023-07-14-llms-just-dont-understand/#a-practical-approach","title":"A Practical Approach","text":"<p>While my alternative terminology table above is admittedly clunky, it serves a purpose: making explicit what we actually mean when we describe AI capabilities.</p> <p>In practice, I find it useful to:</p> <ul> <li>Be specific about what an AI system actually does (processes text, predicts tokens, etc.)</li> <li>Distinguish between performance (what it produces) and process (how it works internally)</li> <li>Acknowledge when using anthropomorphic language for convenience while being clear about the limitations</li> </ul>","tags":["LLM","understanding","anthropomorphism","terminology"]},{"location":"blog/posts/2023-07-14-llms-just-dont-understand/#the-bigger-picture","title":"The Bigger Picture","text":"<p>This linguistic challenge reflects a deeper philosophical question about the nature of intelligence, understanding, and consciousness. As AI systems become more sophisticated, these distinctions may become even more important.</p> <p>For now, I'll continue using terms like \"understanding\" when discussing AI, but with the caveat that I'm describing the external appearance of a capability, not claiming insight into the internal experience (if any) of the system.</p> <p>After all, I'm not even entirely sure what my own \"understanding\" really consists of.</p>","tags":["LLM","understanding","anthropomorphism","terminology"]},{"location":"blog/posts/2023-09-06-llama-cpp-grammars/","title":"Using grammars to constrain llama.cpp output","text":"<p>Context-free grammars have increased the accuracy of my large language model-based biomedical data extraction pipeline.</p> <p>The llama.cpp project, which is a high-performance library for running LLMs locally on CPUs, GPUs, and Apple's Metal graphics platform (e.g M1, M2), has recent added the support of grammars to guide and constrain the output of the LLM.</p> <p>A grammar is a notation that describes the valid syntax of text.</p> <p>The GGML grammar notation (GBNF) is documented here and there are example grammars for generic JSON, C programming language, and chess moves.</p> <p>I have gotten pretty good at crafting my grammars by hand, but these tools are helpful for getting started:</p> <ul> <li>Web UI: Grammar Builder (input: TS types)</li> <li>llama.cpp script: json-schema-to-grammar.py (input: jsonschema file)</li> </ul> <p>For Python usage, this capability was exposed in the llama-cpp-python project starting in version 0.1.78.</p> <p>To use it, there is a class called LlamaGrammar that is passed into your LLM instance on inference:</p> <pre><code>from llama_cpp.llama import Llama, LlamaGrammar\ngrammar = LlamaGrammar.from_string(grammar_text)\nllm = Llama(model_path)\nresponse = llm(prompt, grammar=grammar)\n</code></pre> <p>LlamaGrammar also has a <code>from_file</code> helper function.</p> <p>Grammars work by guiding and constraining the LLM as it is predicting the next token.</p> <p>This feature eliminates the challenges with trying to force the model to generate well-formed JSON via prompt engineering or via post-processing on the response text.</p> <p>In addition to guaranteeing the output, the overall quality and accuracy of the underlying response \"logic\" improves as well. The grammar acts like guardrails in bowling which not only prevents gutter balls (i.e. not well-formed JSON) but also increases the likelihood of a strike (i.e. the correct answer).</p> <p>My biomedical data extraction pipeline extracts drug names, dosages, genomic variants, biomarkers, and clinical findings from unstructured text. Before grammars, I was seeing about 15-20% malformed JSON responses which would require either falling back to a more expensive model or complex parsing logic.</p> <p>With grammars, I get 100% well-formed JSON responses and the accuracy of the extracted data has improved by approximately 25%. This is likely because the model is forced to \"think\" in terms of the structured output format from the beginning of the generation process.</p>","tags":["llama.cpp","grammars","biomedical","data extraction","GBNF"]},{"location":"blog/posts/2023-09-06-llama-cpp-grammars/#example-grammar","title":"Example Grammar","text":"<p>Here's a simplified version of the grammar I use for biomedical entity extraction:</p> <pre><code>root ::= \"{\" ws \"\\\"entities\\\":\" ws \"[\" ws entity_list? ws \"]\" ws \"}\"\n\nentity_list ::= entity (ws \",\" ws entity)*\n\nentity ::= \"{\" ws\n  \"\\\"type\\\":\" ws entity_type ws \",\" ws\n  \"\\\"text\\\":\" ws string ws \",\" ws\n  \"\\\"confidence\\\":\" ws number ws\n  \"}\"\n\nentity_type ::= \"\\\"drug\\\"\" | \"\\\"variant\\\"\" | \"\\\"biomarker\\\"\" | \"\\\"finding\\\"\"\n\nstring ::= \"\\\"\" ([^\"\\\\] | \"\\\\\" .)* \"\\\"\"\nnumber ::= [0-9]+ (\".\" [0-9]+)?\nws ::= [ \\t\\n\\r]*\n</code></pre> <p>This ensures that the output is always a valid JSON object with an \"entities\" array containing properly formatted entity objects.</p>","tags":["llama.cpp","grammars","biomedical","data extraction","GBNF"]},{"location":"blog/posts/2023-09-06-llama-cpp-grammars/#performance-considerations","title":"Performance Considerations","text":"<p>There are some performance implications to consider:</p> <ul> <li>Grammar-constrained generation is slightly slower than unconstrained generation</li> <li>Complex grammars can significantly impact generation speed</li> <li>The grammar parsing adds computational overhead during inference</li> </ul> <p>However, for my use case, the benefits far outweigh the costs:</p> <ul> <li>Eliminated need for retry logic on malformed responses</li> <li>Reduced need for post-processing validation</li> <li>Improved overall pipeline reliability</li> <li>Better extraction accuracy</li> </ul>","tags":["llama.cpp","grammars","biomedical","data extraction","GBNF"]},{"location":"blog/posts/2023-09-06-llama-cpp-grammars/#future-improvements","title":"Future Improvements","text":"<p>I'm exploring several enhancements to my grammar-based approach:</p> <ol> <li>Dynamic grammar generation: Creating grammars programmatically based on the input text</li> <li>Hierarchical grammars: Using different grammars for different types of biomedical documents</li> <li>Grammar optimization: Profiling and optimizing grammars for better performance</li> </ol> <p>If you're working with LLMs for structured data extraction, I highly recommend experimenting with grammars. The initial investment in learning GBNF syntax pays dividends in improved reliability and accuracy.</p>","tags":["llama.cpp","grammars","biomedical","data extraction","GBNF"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/","title":"What is a Custom GPT?","text":"<p>Custom GPTS are a configurable, shareable chat experience available to ChatGPT plus subscribers. Custom GPTs were announced on November 6<sup>th</sup>, 2023 at their inaugural Dev Day event and the GPT Store was announced on January 10<sup>th</sup>, 2024.</p> <p></p> <p>In this article:</p> <ul> <li>What makes up a Custom GPT?</li> <li>Product Announcements Leading up to GPTs</li> <li>Key Benefits of GPTS</li> <li>Key Risks of GPTS</li> <li>Why create a GPT?</li> <li>Further Reading</li> </ul>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/#what-makes-up-a-custom-gpt","title":"What makes up a Custom GPT?","text":"<p>Custom GPTs have the following properties configurable by its creator directly in the GPT Editor:</p> <ul> <li>Name</li> <li>Logo</li> <li>Description</li> <li>Custom Instructions</li> <li>Conversation Starters (max. 4)</li> <li>Knowledge Retrieval<ul> <li>Maximum 10 files</li> <li>Maximum 512MB per file</li> </ul> </li> <li>Optional Access to OpenAI \"Capabilities\":<ul> <li>Web Browsing</li> <li>DALL-E Image Generation</li> <li>Code Interpreter</li> </ul> </li> <li>Actions<ul> <li>Schema (via OpenAPI specification)</li> <li>Authentication Setup (OAuth or Token-based)</li> <li>Privacy Policy URL (for shared GPTs with Actions)</li> </ul> </li> <li>Sharing<ul> <li>Publish: Only you, Anyone with Link, Everyone</li> <li>Public, Shareable URL for non-private GPTs</li> </ul> </li> </ul>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/#product-announcements-leading-up-to-custom-gpts","title":"Product Announcements Leading up to Custom GPTS","text":"<p>ChatGPT Plugins (March 23, 2023): Plugins allowed ChatGPT to call REST API endpoints which demonstrated a very powerful paradigm of language model \"tool usage\". Unfortunately, the discoverability and usability of plugins for both developers and users of plugins was subpar, leading to a lack of product-market fit. Plugins have been refashioned as Actions in GPTs.</p> <p>Code Interpreter (March 23, 2023): Originally released as a plugin, the code interpreter is a sandboxed version of the Python interpreter that enables the creation and running of scripts. Ideally suited as a low-code data analysis tool, it is still available as a GPT capability and as a standalone GPT called Data Analyst.</p> <p>Web Browser (March 23, 2023): Originally released as a plugin, this enabled ChatGPT to search the open web and bring back content for analysis. This capability remains in all standard chats, as a GPT capability and as a standalone GPT called Web Browser.</p> <p>ChatGPT app for iOS (May 18, 2023): The native ChatGPT app was released to iOS in May and rolling out to Android customers starting in July.</p> <p>Advanced Data Analysis (July 6, 2023): The Code Interpreter plugin was renamed to Advanced Data Analysis and made available for all ChatGPT Plus subscribers. The capability was also enabled during plugin \"auto\" mode in combination with other tools.</p> <p>Browse with Bing (September 27, 2023): While the ChatGPT Web Browser plugin was available since March 2023, it was generally unreliable and had been disabled for users who were not browsing. As of September 27, 2023, all ChatGPT Plus and Enterprise users now have web browsing capabilities in default conversations.</p>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/#key-benefits-of-custom-gpts","title":"Key Benefits of Custom GPTS","text":"<p>Improved Discoverability: The GPT Store provides a searchable directory of existing GPTs by category. Something that plugins lacked. You can find GPTs even if you don't know which company created them. For example, you might be searching the Travel category for inspiration without a specific company in mind.</p> <p>Zero-code configuration: Using the GPT Editor provided by OpenAI, developers can provide custom instructions, upload files for knowledge retrieval, and enable capabilities without writing any code.</p> <p>Improved usability for non-developers: Knowledge workers who are not developers can easily create a GPT for a very specific use case. I created the OpenAI CookBook GPT by simply uploading 134 pages of PDF documentation. There's no Python or JavaScript required.</p> <p>Revenue sharing: GPT creators will eventually be paid based on user engagement.</p>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/#key-risks-of-custom-gpts","title":"Key Risks of Custom GPTS","text":"<p>Prompt leaking / Injection: Since the custom instructions are simply prepended to the system prompt, it is generally possible for a user to retrieve the custom instructions by asking the GPT to reveal them via some clever prompting. For commercial or competitive use cases, this may be problematic. More information on prompt leaking.</p> <p>Knowledge extraction: If you upload proprietary files to the knowledge retrieval feature, it may be possible for a user to extract information from your files. For example, there are prompts that can be used to extract the entire content of uploaded files.</p> <p>Limited Action capabilities: OpenAI's Action capabilities are limited to a small set of authentication methods. For example, at launch, GPTs do not support certificate authentication, custom authentication, or complex OAuth flows.</p> <p>No revenue sharing at launch: Although OpenAI has said they will have revenue sharing for GPT creators, this isn't available at launch time.</p>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/#why-create-a-custom-gpt","title":"Why create a Custom GPT?","text":"<p>New tool usage paradigm: From a developer's perspective, tool usage via custom GPTs is by far the easiest way to enable LLM function calling. Instead of needing to write Python or JavaScript code to call APIs, all you need is an OpenAPI schema. This removes a lot of the friction for API providers.</p> <p>Easier than a traditional chatbot: For companies that want to provide a conversational assistant-style interface to their content or APIs, GPTs are by far the easiest way to provide such an experience.</p> <p>New way to deliver software: Because of tool usage capabilities via Actions, you can create very compelling software experiences by combining OpenAI's conversational ability with your own data and APIs. This opens up a new category of \"Conversational Software\".</p> <p>New business model: The eventual revenue sharing opens up a new business model for software creators who can create tools that are used by multiple users and get paid for the usage.</p> <p>Access to 100 million users: By uploading to the GPT Store, you can immediately reach all ChatGPT Plus users who use the GPT Store.</p>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]},{"location":"blog/posts/2024-01-08-what-is-a-custom-gpt/#further-reading","title":"Further Reading","text":"<ul> <li>OpenAI's Blog Post announcing GPTs</li> <li>How to Create Custom ChatGPT Actions with FastAPI</li> <li>OpenAI's GPT Actions Documentation</li> </ul>","tags":["ChatGPT","GPT","Custom GPTs","AI Tools"]}]}